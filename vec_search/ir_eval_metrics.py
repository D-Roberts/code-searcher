"""
This module contains some common IR metrics
for evaluation of the relevance assessments done
by both humans and llms for search queries.

Because the `gen-llm-rels` command reads the input from
a csv generated by `export-rad-to-csv` command we assume all inputs
have that format and are subsequently stored in a
pandas dataframe.

The primary function to use from this module is `calc_ir_metrics` which
will calculate all the supported metrics.

Currently the following metrics are supported:

i. Cohen Kappa
ii. Spearman Correlation
iii. Kendall Tau
iv. Mean ave Precision@k
"""
from typing import Union

from scipy.stats import spearmanr, kendalltau
from pandas import DataFrame, crosstab


def cohen_kappa(df: DataFrame) -> dict[str, float]:
    """Calculate Cohen Kappa for binary relevances.
    cf. https://nlp.stanford.edu/IR-book/pdf/08eval.pdf
    Args:
        df (DataFrame): A pandas dataframe that contains 2 binary relevance columns

    Returns:
        dict[str, float]: The calculated Cohen Kappa statistic
    """
    x_tab = crosstab(df.relevances, df.llm_rel_score, margins=True)
    # because of 0 on off diag we use a hacky good-turing smoother
    # TODO: make this work for ordinal >2 relevances
    p_yes = x_tab[1][1] + 1
    p_no = x_tab[0][0] + 1
    n_items = x_tab['All']['All']
    p_A =  (p_yes + p_no) / n_items
    p_rel = (x_tab[1]['All'] + x_tab['All'][1]) / (2 * n_items)
    p_nrel = (x_tab[0]['All'] + x_tab['All'][0]) / (2 * n_items)
    p_E = p_rel * p_rel + p_nrel * p_nrel
    return {'Cohen-Kappa': (p_A - p_E) / (1. - p_E)}

def spearman_corr(df: DataFrame) -> dict[str, float]:
    """Calculate Spearman correlation

    Args:
        df (DataFrame):  A pandas dataframe that contains 2 relevance columns.

    Returns:
        dict[str, float]: Calculated correlation and corresponding p-value
    """
    s_corr, s_pv = spearmanr(df.relevances, df.llm_rel_score)
    return {'Spearman-corr': s_corr, 'Spearman-p-value': s_pv}

def kendall_tau(df: DataFrame) -> dict[str, float]:
    """Calculate Kendall Tau

    Args:
        df (DataFrame): A pandas dataframe that contains 2 relevance columns.

    Returns:
        dict[str, float]: Calculated correlation and corresponding p-value
    """
    k_tau, k_pv = kendalltau(df.relevances, df.llm_rel_score)
    return {'Kendall-tau': k_tau, 'Kendall-p-value': k_pv}


def mean_ave_prec(df: DataFrame, k: int=10) -> dict[str, float]:
    """ Computes the mean average precision at k.

    cf. https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision
    Args:
        df (DataFrame): _description_
        k (int, optional): _description_. Defaults to 10.

    Returns:
        dict[str, float]: _description_
    """
    column_names = ['llm_rel_score', 'relevances', 'query_id', 'rank']
    # `grp` here is the query id and `adf` is the dataframe
    actuals, predicteds = [], []
    for grp, adf in df[column_names].groupby('query_id'):
        print("\n\n", grp, "\n\n")
        breakpoint()


def calc_ir_metrics(df: DataFrame, functions: list[callable]=[]) -> dict[str, float]:
    """A wrapper around the metrics supported by the module.
    The intent is for this function to be used in the click command, `gen-ir-metrics`.

    Args:
        df (DataFrame): The dataframe to calculate metrics on.
        functions (list[callable], optional): A list of additional metrics to calculate.
            Defaults to [].

    Returns:
        dict[str, float]: A dictionary of the calculated metrics.
    """
    stats = {}
    funcs = [cohen_kappa, spearman_corr, kendall_tau]
    if functions:
        funcs.extend(functions)
    for func in funcs:
        stats.update(func(df))
    return stats

if __name__ == "__main__":
    import unittest

    # TODO: add pytest to the dependencies and use the annotation there
    #  to simplify the manual looping in these test cases...
    class TestMeanAveragePrecision(unittest.TestCase):
        def test_mapk_no_correct(self):
            pass

        def test_mapk_one_correct_equals_reciprocal_rank(self):
            pass

        def test_mapk_more_than_one_correct(self):
            pass

    
    # the main method here invokes discovery, execution, etc.
    unittest.main()