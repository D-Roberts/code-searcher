"""
This module contains some common IR metrics
for evaluation of the relevance assessments done
by both humans and llms for search queries.

Because the `gen-llm-rels` command reads the input from
a csv generated by `export-rad-to-csv` command we assume all inputs
have that format and are subsequently stored in a
pandas dataframe.

The primary function to use from this module is `calc_ir_metrics` which
will calculate all the supported metrics.

Currently the following metrics are supported:

i. Cohen Kappa
ii. Spearman Correlation
iii. Kendall Tau
iv. Mean ave Precision@k
"""
from typing import Union

from scipy.stats import spearmanr, kendalltau
from pandas import DataFrame, crosstab


def cohen_kappa(df: DataFrame) -> dict[str, float]:
    """Calculate Cohen Kappa for binary relevances.
    cf. https://nlp.stanford.edu/IR-book/pdf/08eval.pdf
    Args:
        df (DataFrame): A pandas dataframe that contains 2 binary relevance columns

    Returns:
        dict[str, float]: The calculated Cohen Kappa statistic
    """
    x_tab = crosstab(df.relevances, df.llm_rel_score, margins=True)
    # because of 0 on off diag we use a hacky good-turing smoother
    # TODO: make this work for ordinal >2 relevances
    p_yes = x_tab[1][1] + 1
    p_no = x_tab[0][0] + 1
    n_items = x_tab['All']['All']
    p_A =  (p_yes + p_no) / n_items
    p_rel = (x_tab[1]['All'] + x_tab['All'][1]) / (2 * n_items)
    p_nrel = (x_tab[0]['All'] + x_tab['All'][0]) / (2 * n_items)
    p_E = p_rel * p_rel + p_nrel * p_nrel
    return {'Cohen-Kappa': (p_A - p_E) / (1. - p_E)}

def spearman_corr(df: DataFrame) -> dict[str, float]:
    """Calculate Spearman correlation

    Args:
        df (DataFrame):  A pandas dataframe that contains 2 relevance columns.

    Returns:
        dict[str, float]: Calculated correlation and corresponding p-value
    """
    s_corr, s_pv = spearmanr(df.relevances, df.llm_rel_score)
    return {'Spearman-corr': s_corr, 'Spearman-p-value': s_pv}

def kendall_tau(df: DataFrame) -> dict[str, float]:
    """Calculate Kendall Tau

    Args:
        df (DataFrame): A pandas dataframe that contains 2 relevance columns.

    Returns:
        dict[str, float]: Calculated correlation and corresponding p-value
    """
    k_tau, k_pv = kendalltau(df.relevances, df.llm_rel_score)
    return {'Kendall-tau': k_tau, 'Kendall-p-value': k_pv}


def _apk(actual, predicted, k=10):
    """
    Computes the average precision at k.

    This function computes the average prescision at k between two lists of
    items.

    Args:
        actual : list
                 A list of elements that are to be predicted (order doesn't matter)
        predicted : list
                    A list of predicted elements (order does matter)
        k : int, optional
            The maximum number of predicted elements, 10 is the default.

    Returns:
        score : dict[str, float]
                The average precision at k over the input lists.
    """
    if len(predicted)>k:
        predicted = predicted[:k]
    score = 0.0
    num_hits = 0.0
    for i,p in enumerate(predicted):
        if p in actual and p not in predicted[:i]:
            num_hits += 1.0
            score += num_hits / (i+1.0)
    if not actual:
        return 0.0
    return {'apk' : score / min(len(actual), k)}

def _mapk(actual, predicted, k=10):
    """
    Computes the mean average precision at k.

    This is an internal function-do not use.

    This function computes the mean average prescision at k between two lists
    of lists of items.

    Args:
        actual : list
                 A list of lists of elements that are to be predicted
                 (order doesn't matter in the lists)
        predicted : list
                    A list of lists of predicted elements
                    (order matters in the lists)
        k : int, optional
            The maximum number of predicted elements, 10 is the default.

    Returns:
        score : dict[str, float]
                The mean average precision at k over the input lists
    """
    assert len(actual) == len(predicted), "error: len(actual) != len(predicted)..."
    return sum([_apk(a,p,k)['apk'] for a,p in zip(actual, predicted)]) / len(actual)

def mean_ave_prec(df: DataFrame, k: int=10) -> dict[str, float]:
    """ Computes the mean average precision at k.

    Args:
        df (DataFrame): _description_
        k (int, optional): _description_. Defaults to 10.

    Returns:
        dict[str, float]: _description_
    """
    column_names = ['llm_rel_score', 'relevances', 'query_id', 'rank']
    # `grp` here is the query id and `adf` is the dataframe
    actuals, predicteds = [], []
    for grp, adf in df[column_names].groupby('query_id'):
        print("\n\n", grp, "\n\n")
        adf[]

def calc_ir_metrics(df: DataFrame, functions: list[callable]=[]) -> dict[str, float]:
    """A wrapper around the metrics supported by the module.
    The intent is for this function to be used in the click command, `gen-ir-metrics`.

    Args:
        df (DataFrame): The dataframe to calculate metrics on.
        functions (list[callable], optional): A list of additional metrics to calculate.
            Defaults to [].

    Returns:
        dict[str, float]: A dictionary of the calculated metrics.
    """
    stats = {}
    funcs = [cohen_kappa, spearman_corr, kendall_tau]
    if functions:
        funcs.extend(functions)
    for func in funcs:
        stats.update(func(df))
    breakpoint()
    return stats

if __name__ == "__main__":
    import unittest

    # TODO: add pytest to the dependencies and use the annotation there
    #  to simplify the manual looping in these test cases...
    class TestAveragePrecision(unittest.TestCase):
        def test_apk(self):
            apk = _apk(range(1,6),[6,4,7,1,2], 2)
            self.assertAlmostEqual(apk['apk'], 0.25)
            apk = _apk(range(1,6),[1,1,1,1,1], 5)
            self.assertAlmostEqual(apk['apk'], 0.2)
            predicted = list(range(1,21))
            predicted.extend(range(200,600))
            apk = _apk(range(1,100),predicted, 20)
            self.assertAlmostEqual(apk['apk'], 1.0)

        def test_mapk(self):
            self.assertAlmostEqual(_mapk([range(1,5)],[range(1,5)],3), 1.0)
            # can have multiple lists via nesting...
            self.assertAlmostEqual(_mapk([[1,3,4],[1,2,4],[1,3]],
                [range(1,6),range(1,6),range(1,6)], 3), 0.685185185185185)
            self.assertAlmostEqual(_mapk([range(1,6),range(1,6)],
                [[6,4,7,1,2],[1,1,1,1,1]], 5), 0.26)
            self.assertAlmostEqual(_mapk([[1,3],[1,2,3],[1,2,3]],
                [range(1,6),[1,1,1],[1,2,1]], 3), 11.0/18)
    
    # the main method here invokes discovery, execution, etc.
    unittest.main()